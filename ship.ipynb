import numpy as np
from numpy import expand_dims
import pandas as pd

import json
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
from tensorflow.keras.utils import to_categorical
import keras
from keras import layers
from keras.wrappers.scikit_learn import KerasClassifier
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from tensorflow.keras.optimizers import RMSprop,Adam
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import EarlyStopping

with open('shipsnet.json') as data_file:
    dataset = json.load(data_file)
shipsnet= pd.DataFrame(dataset)
shipsnet.head()

shipsnet.info()

shipsnet = shipsnet[["data", "labels"]]
shipsnet.head()

ship_images = shipsnet["labels"].value_counts()[0] #if there is ship
no_ship_images = shipsnet["labels"].value_counts()[1] #if there is no ship
print("Number of the ship_images :{}".format(ship_images),"\n")
print("Number of the no ship_images :{}".format(no_ship_images))

x = np.array(dataset['data']).astype('uint8')
y = np.array(dataset['labels']).astype('uint8')

#shape of data
#The current data for each image is one row of flattened 19200 data points representing the RGB values of each pixel.
x.shape

#After reshaping, each item in new x variable will be 3 lists. 
#Each of these lists will be RGB values for each pixel for the length and width of the image.
x_reshaped = x.reshape([-1, 3, 80, 80])

x_reshaped.shape
x_reshaped = x.reshape([-1, 3, 80, 80]).transpose([0,2,3,1])
x_reshaped.shape

#converting to matrix..
y_reshaped = to_categorical(y, num_classes=2)

y_reshaped.shape
y_reshaped

image_no_ship = x_reshaped[y==0]
image_ship = x_reshaped[y==1]

def plot(a,b):
    
    plt.figure(figsize=(15, 15))
    for i, k in enumerate(range(1,9)):
        if i < 4:
            plt.subplot(2,4,k)
            plt.title('Not A Ship')
            plt.imshow(image_no_ship[i+2])
            plt.axis("off")#turn off the axis..
        else:
            plt.subplot(2,4,k)
            plt.title('Ship')
            plt.imshow(image_ship[i+15])
            plt.axis("off")
            
    plt.subplots_adjust(bottom=0.3, top=0.7, hspace=0.25)

#Implementation of the function 

plot(image_no_ship, image_ship)

image_no_ship = x_reshaped[y==0]
image_ship = x_reshaped[y==1]
def plotHistogram(ship, not_ship):

    plt.figure(figsize = (10,7))
    plt.subplot(2,2,1)#the figure has (number of  row, number of  columns, index of plot.)
    plt.imshow(ship)#The imshow() function in pyplot module of matplotlib library is used to display data as an image
    plt.axis('off')
    plt.title('Ship')
    histo = plt.subplot(2,2,2)
    histo.set_ylabel('Count', fontweight = "bold")
    histo.set_xlabel('Pixel Intensity', fontweight = "bold")
    n_bins = 30 #The height of each bin shows how many values from that data fall into that range. 
    #alpha to be very useful when comparing histograms of several distributions:
    plt.hist(ship[:,:,0].flatten(), bins = n_bins, lw = 0, color = 'r', alpha = 0.5);#lw=linewidth
    plt.hist(ship[:,:,1].flatten(), bins = n_bins, lw = 0, color = 'g', alpha = 0.5);
    #he alpha channel is not necessary here, but it can be used to soften colors for more visually appealing plots.
    plt.hist(ship[:,:,2].flatten(), bins = n_bins, lw = 0, color = 'b', alpha = 0.5);
    plt.show()
    print("Minimum pixel value of this image: {}".format(ship.min()))
    print("Maximum pixel value of this image: {}".format(ship.max()))
    plt.figure(figsize = (10,7))
    plt.subplot(2,2,3)
    plt.imshow(not_ship)
    plt.axis('off')
    plt.title('Not A Ship')
    histo = plt.subplot(2,2,4)
    histo.set_ylabel('Count', fontweight = "bold")
    histo.set_xlabel('Pixel Intensity', fontweight = "bold")
    n_bins = 30
    plt.hist(not_ship[:,:,0].flatten(), bins = n_bins, lw = 0, color = 'r', alpha = 0.5);
    plt.hist(not_ship[:,:,1].flatten(), bins = n_bins, lw = 0, color = 'g', alpha = 0.5);
    plt.hist(not_ship[:,:,2].flatten(), bins = n_bins, lw = 0, color = 'b', alpha = 0.5);
    plt.show()
    print("Minimum pixel value of this image: {}".format(not_ship.min()))
    print("Maximum pixel value of this image: {}".format(not_ship.max()))

for i in range (20,21):
    plotHistogram(x_reshaped[y==1][i], x_reshaped[y==0][i])

my_list = [(0, 'R channel'), (1, 'G channel'), (2, 'B channel')]

plt.figure(figsize = (15,15))

for i, k in my_list:
    plt.subplot(1,3,i+1)
    plt.title(k)
    plt.ylabel('Height {}'.format(x_reshaped[y==0][5].shape[0]))
    plt.xlabel('Width {}'.format(x_reshaped[y==0][5].shape[1]))
    plt.imshow(x_reshaped[y==1][5][ : , : , i])

x_reshaped = x_reshaped / 255
x_reshaped[0][0][0]


n_bins = 30
plt.hist(x_reshaped[y == 0][0][:,:,0].flatten(), bins = n_bins, lw = 0, color = 'r', alpha = 0.5);
plt.hist(x_reshaped[y == 0][0][:,:,1].flatten(), bins = n_bins, lw = 0, color = 'g', alpha = 0.5);
plt.hist(x_reshaped[y == 0][0][:,:,2].flatten(), bins = n_bins, lw = 0, color = 'b', alpha = 0.5);
plt.ylabel('Count', fontweight = "bold")
plt.xlabel('Pixel Intensity', fontweight = "bold")
plt.title("Histogram of normalized data")
plt.show()

#train_test_split is a function in Sklearn model selection for splitting data arrays into two subsets: 
#for training data and for testing data. With this function, you don't need to divide the dataset manually.

#By default, Sklearn train_test_split will make random partitions for the two subsets. However, you can also specify a random state for the operation. 
x_train_1, x_test, y_train_1, y_test = train_test_split(x_reshaped, y_reshaped,
                                                        test_size = 0.20, random_state = 42)

#size of the testing dataset. The default state suits the training size. 
#It will be set to 0.25 if the training size is set to default.

x_train, x_val, y_train, y_val = train_test_split(x_train_1, y_train_1, 
                                                  test_size = 0.25, random_state = 42)


print("x_train shape",x_train.shape)
print("x_test shape",x_test.shape)
print("y_train shape",y_train.shape)
print("y_test shape",y_test.shape)
print("y_train shape",x_val.shape)
print("y_test shape",y_val.shape)

from keras import callbacks
model = Sequential()
model.add(Flatten(input_shape=[80, 80, 3]))
model.add(Dense(200, activation='relu'))
model.add(Dense(150, activation='relu'))
model.add(Dense(2, activation='sigmoid'))

model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])
earlystopping = callbacks.EarlyStopping(monitor ="val_loss", 
                                        mode ="min", patience = 10, 
                                        restore_best_weights = True)

history = model.fit(x_train, y_train, epochs = 100, validation_data=(x_val, y_val), callbacks = [earlystopping])

prediction = model.predict(x_test)
pd.Series(prediction[0], index=["Not A Ship", "Ship"])

plt.imshow(x_test[0])
plt.axis("off")
plt.show()

predicted_data = pd.DataFrame(prediction, columns=["Not A Ship", "Ship"])
predicted_data.head(20)

y_test_data = pd.DataFrame(y_test, columns=["Not A Ship", "Ship"])
y_test_data.head(20)

predicted_data['There is a Ship'] = y_test[:, 1]
predicted_data.head(20)

predicted_data["Difference"] = predicted_data["Ship"] - predicted_data["There is a Ship"]
predicted_data

indexes = predicted_data.sort_values('Difference', ascending = False).head(4).index.to_list()

def plotHistogram(image_index):

    plt.figure(figsize = (10,7))
    plt.subplot(2,2,1)
    plt.imshow(x_test[image_index])
    plt.axis('off')
    plt.title('There is no ship. But predicted as a ship.')
    histo = plt.subplot(2,2,2)
    histo.set_ylabel('Count', fontweight = "bold")
    histo.set_xlabel('Pixel Intensity', fontweight = "bold")
    n_bins = 30
    plt.hist(x_test[image_index][:,:,0].flatten(), bins = n_bins, lw = 0, color = 'r', alpha = 0.5);
    plt.hist(x_test[image_index][:,:,1].flatten(), bins = n_bins, lw = 0, color = 'g', alpha = 0.5);
    plt.hist(x_test[image_index][:,:,2].flatten(), bins = n_bins, lw = 0, color = 'b', alpha = 0.5);
    plt.show()


#Implementation of the function

for i in indexes:
    plotHistogram(i)


indexes = predicted_data.sort_values('Difference', ascending = True).head(4).index.to_list()

def plotHistogram(image_index):

    plt.figure(figsize = (10,7))
    plt.subplot(2,2,1)
    plt.imshow(x_test[image_index])
    plt.axis('off')
    plt.title('There is a ship. But predicted as not a ship.')
    histo = plt.subplot(2,2,2)
    histo.set_ylabel('Count', fontweight = "bold")
    histo.set_xlabel('Pixel Intensity', fontweight = "bold")
    n_bins = 30
    plt.hist(x_test[image_index][:,:,0].flatten(), bins = n_bins, lw = 0, color = 'r', alpha = 0.5);
    plt.hist(x_test[image_index][:,:,1].flatten(), bins = n_bins, lw = 0, color = 'g', alpha = 0.5);
    plt.hist(x_test[image_index][:,:,2].flatten(), bins = n_bins, lw = 0, color = 'b', alpha = 0.5);
    plt.show()


#Implementation of the function

for i in indexes:
    plotHistogram(i)

#Introduction to CNN
#Convolutional Neural Network is a Deep Learning algorithm specially designed for working with Images and videos.
#It takes images as inputs, extracts and learns the features of the image, and classifies them based on the learned features.
from keras import callbacks
model = Sequential()
#The CNN model works in two steps: feature extraction and Classification
#Conv2D parameter is the number of filters that the convolutional layer will learn.
#Padding = output volume size matches the input volume size
model.add(Conv2D(filters = 64, kernel_size = (4,4),padding = 'Same', 
                 activation ='relu', input_shape = (80,80,3)))
#kernel_size is a  2-tuple specifying the width and height of the 2D convolution window.
#total of 64 filters.
model.add(MaxPool2D(pool_size=(5,5))) 
#Max pooling is then used to reduce the spatial dimensions of the output volume. 
#MaxPool2D Using pooling, a lower resolution version of input
#is created that still contains the large or important elements of the input image.
#model.add(Dropout(0.25))
#total of 32 filters.
model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', 
                 activation ='relu'))
#Stride denotes how many steps we are moving in each steps in convolution.By default it is one.
model.add(MaxPool2D(pool_size=(3,3), strides=(1,1)))
#model.add(Dropout(0.25))
#total of 16 filters.
model.add(Conv2D(filters = 16, kernel_size = (2,2),padding = 'Same', 
                 activation ='relu'))
model.add(MaxPool2D(pool_size=(3,3), strides=(1,1)))
#model.add(Dropout(0.25))

# Fully connected

#flatten function flattens the multi-dimensional input tensors into a single dimension,
#so you can model your input layer and build your neural network model
model.add(Flatten())
#Dense function is used to create fully connected layers, in which every output depends on every input
model.add(Dense(200, activation = "relu"))#200 is the size of the output from the dense layer
model.add(Dropout(0.5))
model.add(Dense(100, activation = "relu"))#100 is the size of the output from the dense layer
model.add(Dropout(0.5))
model.add(Dense(100, activation = "relu"))
model.add(Dropout(0.5))
model.add(Dense(50, activation = "relu"))#50 is the size of the output from the dense layer
model.add(Dropout(0.5))
model.add(Dense(2, activation = "softmax"))

#Optimizers are the expanded class, which includes the method to train your machine learning model. 
#Right optimizers are necessary for your model as they improve training speed and performance
optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)
#Adam stands for adaptive moment estimation
#Adam utilizes the concept of momentum by adding fractions of previous gradients to the current one.

model.compile(optimizer = optimizer , loss = "categorical_crossentropy", metrics=["accuracy"])

#Early stopping is a method that allows you to  specify an arbitrarily
# large number of training epochs and stop training once the model performance stops improving on the validation dataset.
earlystopping = callbacks.EarlyStopping(monitor ="val_loss", 
                                        mode ="min", patience = 10, 
                                        restore_best_weights = True)
history = model.fit(x_train, y_train, epochs = 100, validation_data=(x_val, y_val), callbacks = [earlystopping])

 model.evaluate(x_test, y_test)

pd.DataFrame(history.history).plot();

history = model.fit(datagen.flow(x_train, y_train), epochs = 100, 
                    validation_data=(x_val, y_val), callbacks = [earlystopping])

model.evaluate(x_test, y_test)

from sklearn import metrics
import seaborn as sns
Y_pred = model.predict(x_test)
# Convert predictions classes to one  vectors 
Y_pred_classes = np.argmax(Y_pred,axis = 1) 
# Convert validation observations to one  vectors
Y_true = np.argmax(y_test,axis = 1) 
# Compute the confusion matrix

print("\n""Test Accuracy Score : ",metrics.accuracy_score(Y_true, Y_pred_classes),"\n")

fig, axis = plt.subplots(1, 3, figsize=(20,6))
axis[0].plot(history.history['val_accuracy'], label='val_acc')
axis[0].set_title("Validation Accuracy")
axis[0].set_xlabel("Epochs")
axis[0].set_ylabel("Val. Acc.")
axis[1].plot(history.history['accuracy'], label='acc')
axis[1].set_title("Training Accuracy")
axis[1].set_xlabel("Epochs")
axis[0].set_ylabel("Train. Acc.")
axis[2].plot(history.history['val_loss'], label='val_loss')
axis[2].set_title("Test Loss")
axis[2].set_xlabel("Epochs")
axis[2].set_ylabel("Loss")

plt.show()

confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) 
# Plot the confusion matrix
f,ax = plt.subplots(figsize=(7, 7))
sns.heatmap(confusion_mtx, annot=True, linewidths=0.01,linecolor="gray", fmt= '.1f',ax=ax, annot_kws={'size':25})
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.tight_layout()
print(Y_true.dtype)
print(Y_pred_classes.dtype)
print(confusion_mtx)

plt.show()


